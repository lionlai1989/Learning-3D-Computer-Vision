{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Representations\n",
    "\n",
    "In the realm of 3D computer vision, solid objects are typically represented using one of\n",
    "two main categories:\n",
    "\n",
    "-   **Boundary Representations:** This category encompasses methods that define the\n",
    "    limits or edges of objects. It includes:\n",
    "\n",
    "    -   **Point Clouds:** A collection of points in a 3D space. Each point represents a\n",
    "        vertex of the object's surface.\n",
    "    -   **Meshes:** Composed of vertices, edges, and faces that define the shape of a 3D\n",
    "        object. Meshes provide a more detailed representation than point clouds, as they\n",
    "        include information on how these points are connected.\n",
    "    -   **Parametric Surfaces:** Surfaces defined by parametric equations. They offer a\n",
    "        smooth representation and can be used to model complex shapes with fewer data\n",
    "        points than meshes.\n",
    "\n",
    "-   **Space-partitioning Representations:** These methods divide the 3D space into\n",
    "    discrete segments to represent the volume of objects. They include:\n",
    "\n",
    "    -   **Voxels:** The 3D equivalent of pixels. A voxel represents a value on a regular\n",
    "        grid in three-dimensional space. Voxels are particularly useful for representing\n",
    "        volumetric data.\n",
    "    -   **Implicit Surfaces:** Defined by a function that specifies whether a point in\n",
    "        space is inside or outside the object. These surfaces are powerful for modeling\n",
    "        smooth, continuous surfaces and complex topologies.\n",
    "\n",
    "This tutorial aims to explore the rendering of voxels, meshes, and point clouds using\n",
    "Python packages, with a primary focus on PyTorch3D. PyTorch3D is a versatile library\n",
    "designed to support 3D data processing tasks, offering efficient implementations for\n",
    "various 3D representations.\n",
    "\n",
    "The aim of this notebook is to use Python packages, primarily PyTorch3D, for rendering\n",
    "voxels, meshes, and point clouds.\n",
    "\n",
    "**TODO:** Add Parametric Surfaces and Implicit Surfaces into this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import functools\n",
    "from pprint import pprint\n",
    "import gc\n",
    "\n",
    "import imageio\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch3d.io import load_obj, load_objs_as_meshes\n",
    "from pytorch3d.renderer.blending import BlendParams\n",
    "from pytorch3d.renderer.lighting import PointLights\n",
    "from pytorch3d.renderer.materials import Materials\n",
    "from pytorch3d.renderer.mesh.renderer import MeshRenderer\n",
    "from pytorch3d.renderer.mesh.rasterizer import MeshRasterizer, RasterizationSettings\n",
    "from pytorch3d.renderer.mesh.textures import TexturesVertex, Textures\n",
    "from pytorch3d.renderer.mesh.shader import SoftPhongShader\n",
    "from pytorch3d.renderer.points.renderer import PointsRenderer\n",
    "from pytorch3d.renderer.points.rasterizer import (\n",
    "    PointsRasterizer,\n",
    "    PointsRasterizationSettings,\n",
    ")\n",
    "from pytorch3d.renderer.points.compositor import AlphaCompositor\n",
    "from pytorch3d.renderer.implicit.renderer import VolumeRenderer\n",
    "from pytorch3d.renderer.implicit.raysampling import NDCMultinomialRaysampler\n",
    "from pytorch3d.renderer.implicit.raymarching import EmissionAbsorptionRaymarcher\n",
    "from pytorch3d.renderer.cameras import (\n",
    "    look_at_view_transform,\n",
    "    look_at_rotation,\n",
    "    FoVOrthographicCameras,\n",
    "    FoVPerspectiveCameras,\n",
    ")\n",
    "from pytorch3d.structures import Meshes, Pointclouds, Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device Selection:** It's preferable to use a GPU for running this notebook. With GPU\n",
    "acceleration, each block can complete in tens of seconds or around one to two minutes at\n",
    "most. However, if a CPU is used, it may take three to five times longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "OUTPUT_DIR = Path(\"./output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Systems in PyTorch3D\n",
    "\n",
    "PyTorch3D works with four primary coordinate systems to manage 3D objects and camera\n",
    "perspectives. These coordinate systems are:\n",
    "\n",
    "1. World Coordinates: This system defines the position and orientation of objects in a\n",
    "   3D scene globally. It is the universal frame of reference, where each object's\n",
    "   location is specified relative to a common origin point.\n",
    "\n",
    "2. Camera Coordinates (View Coordinates): Camera coordinates position objects relative\n",
    "   to the camera's viewpoint. In this system, objects are transformed from the world\n",
    "   coordinates to a viewpoint where the camera is at the origin, aiming along a specific\n",
    "   axis (commonly the negative z-axis). This transformation facilitates the rendering\n",
    "   process by aligning objects according to the camera's perspective.\n",
    "\n",
    "3. NDC (Normalized Device Coordinates): After defining objects in the camera's\n",
    "   perspective, the coordinates are normalized to a unit cube before rasterization. The\n",
    "   NDC system ensures that coordinates are within a standard range (typically from -1 to\n",
    "   1 in all three axes), making it easier to map them to the 2D screen space.\n",
    "\n",
    "4. Screen Coordinates (Image Coordinates): The final transformation maps NDC to the\n",
    "   actual pixel coordinates on the screen or viewport. This step adjusts for the aspect\n",
    "   ratio and dimensions of the output medium, translating and scaling the normalized\n",
    "   coordinates to fit the display window or image.\n",
    "\n",
    "### Camera Models in PyTorch3D\n",
    "\n",
    "PyTorch3D offers four camera models to simulate various photographic and viewing\n",
    "effects. The available camera models are:\n",
    "\n",
    "1. `FoVPerspectiveCameras`: This model simulates a field of view (FoV) perspective\n",
    "   camera, which mimics the way human eyes and traditional cameras perceive depth.\n",
    "   Objects closer to the camera appear larger than those further away, creating a\n",
    "   realistic sense of depth. The field of view can be adjusted to widen or narrow the\n",
    "   camera's visible scene.\n",
    "\n",
    "2. `FoVOrthographicCameras`: Combining field of view settings with an orthographic\n",
    "   projection, this camera model removes perspective distortion. It is useful for cases\n",
    "   where maintaining the true dimensions of objects regardless of depth is important,\n",
    "   such as in architectural visualization.\n",
    "\n",
    "3. `PerspectiveCameras`: A standard perspective camera model without explicit field of\n",
    "   view parameters. Instead, it uses focal length and principal point parameters to\n",
    "   define the camera's perspective projection. This model is versatile for general 3D\n",
    "   rendering tasks where realistic depth perception is desired.\n",
    "\n",
    "4. `OrthographicCameras`: This model projects 3D objects onto the 2D screen without\n",
    "   perspective distortion, meaning objects retain their size regardless of their\n",
    "   distance from the camera. It is ideal for technical and engineering applications\n",
    "   where accurate dimensions are critical.\n",
    "\n",
    "**TODO:** Read\n",
    "[this](https://www.mathworks.com/help/vision/ug/fisheye-calibration-basics.html) and\n",
    "[this](https://github.com/Gil-Mor/iFish/tree/master) to understand more about fish-eye\n",
    "model and implement fish-eye intrinsic parameters in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meshes Representation\n",
    "\n",
    "Meshes are 2D surfaces that collectively form the outer shape of a 3D object. These\n",
    "meshes can either be composed of triangles or quadrilaterals. However, due to their\n",
    "simplicity and versatility, triangular meshes are most commonly used in 3D computer\n",
    "vision and graphics. A mesh is primarily composed of two components: the coordinates of\n",
    "its vertices and the indices of these vertices that form a triangle.\n",
    "\n",
    "Each triangle in a mesh is defined by three vertices, and these vertices are identified\n",
    "by their positions in 3D space, typically represented as (x, y, z) coordinates. The\n",
    "indices of these vertices are used to uniquely identify each triangle, facilitating\n",
    "efficient storage and rendering by graphics hardware.\n",
    "\n",
    "**Textures:**\n",
    "\n",
    "Textures can significantly enhance the visual appearance of meshes. By mapping a 2D\n",
    "image onto the surface of a 3D model, textures add details such as colors, patterns, and\n",
    "real-world appearance to the object without increasing the complexity of the mesh. This\n",
    "process involves defining UV coordinates for each vertex, which correspond to specific\n",
    "points on the 2D texture image. The rendering system then interpolates these coordinates\n",
    "across the surface of the triangles to apply the texture.\n",
    "\n",
    "**Additional Features:**\n",
    "\n",
    "Beyond the basic structure and texturing, meshes can include several additional features\n",
    "that enhance their realism and utility:\n",
    "\n",
    "-   **Normal Vectors:** Each vertex or face in a mesh can have a normal vector, which is\n",
    "    a unit vector pointing perpendicular to the surface. Normal vectors are crucial for\n",
    "    lighting calculations, as they determine how light reflects off surfaces, affecting\n",
    "    the appearance of the mesh under different lighting conditions.\n",
    "\n",
    "-   **Vertex Colors:** While textures provide detailed surface appearance, vertex colors\n",
    "    allow for the coloring of individual vertices. The colors of the vertices are then\n",
    "    interpolated across the faces of the mesh, enabling a gradient effect or simple\n",
    "    color variations without the need for a texture.\n",
    "\n",
    "In this tutorial, the `./data/IronMan` folder contains the following files:\n",
    "\n",
    "```none\n",
    "IronMan.mtl\n",
    "IronMan.obj\n",
    "v1_0_IronManRigged.max\n",
    "```\n",
    "\n",
    "Blender can import the `IronMan.obj` file and render the Iron Man model pretty well as\n",
    "shown in the image below. However, I haven't figured out how to integrate the colors\n",
    "using PyTorch3D.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./blender_ironman_screenshot.png\" style=\"width: 512px; height: auto;\">\n",
    "      <p><strong>The Iron Man</strong></p>\n",
    "      <p>Upon importing `IronMan.obj` into Blender, the software correctly uses the textures from the associated files, showing a detailed 3D representation of Iron Man. This mesh is adapted from \"https://free3d.com/\".</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "**TODO:** Incorporate the native textures of the \"IronMan.obj\" into rendering with\n",
    "PyTorch3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_filename = DATA_DIR / \"IronMan/IronMan.obj\"\n",
    "\n",
    "vertices, faces, aux = load_obj(\n",
    "    f=mesh_filename,\n",
    "    load_textures=True,\n",
    "    create_texture_atlas=True,\n",
    "    texture_atlas_size=4,\n",
    "    texture_wrap=\"repeat\",\n",
    "    device=device,\n",
    ")\n",
    "print(\"vertices: \", type(vertices), vertices.shape)\n",
    "print(\"faces: \", type(faces))\n",
    "pprint(dir(faces))\n",
    "\n",
    "# TODO: Figure out how to use the following textures to render prettier Iron Man.\n",
    "print(\"aux.normals: \", aux.normals.shape)\n",
    "print(\"aux.verts_uvs: \", aux.verts_uvs.shape)\n",
    "print(\"aux.material_colors: \")\n",
    "pprint(aux.material_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Textures for the Iron Man Model\n",
    "\n",
    "To address the difficulty mentioned previously, I will manually create artificial\n",
    "textures for the Iron Man model. I have selected a reddish color, represented by the RGB\n",
    "values `[1.0, 0.5, 0.5]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use pytorch3d.renderer.mesh.textures.Textures to construct colorful textures.\n",
    "# Read https://pytorch3d.readthedocs.io/en/v0.6.0/notes/meshes_io.html?highlight=Meshes\n",
    "color = [1.0, 0.5, 0.5]\n",
    "textures = torch.tensor(color).to(device) * torch.ones_like(vertices)  # (num_vertices, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an `Meshes` object and rescaling the meshes.\n",
    "\n",
    "We are now ready to create a `Meshes` object. When rendering an object of unknown size,\n",
    "it is highly convenient to initially reposition the object's center to `(0, 0, 0)` and\n",
    "adjust its scale so that its dimensions fall within the range of `[-1, 1]`. This\n",
    "standardization simplifies subsequent rendering and manipulation tasks by ensuring\n",
    "uniformity across different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Meshes(\n",
    "    verts=[vertices],\n",
    "    faces=[faces.verts_idx],\n",
    "    textures=TexturesVertex([textures]),\n",
    ")\n",
    "\n",
    "verts = mesh.verts_packed()\n",
    "print(f\"The shape of the packed vertices is {verts.shape}\")\n",
    "\n",
    "center = verts.mean(dim=0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "\n",
    "print(\"\\nBefore moving and rescaling:\")\n",
    "print(\"Center: \", center)\n",
    "print(\"Scale: \", verts.min(), \"to\", verts.max())\n",
    "\n",
    "mesh.offset_verts_(-center)\n",
    "mesh.scale_verts_((1.0 / float(scale)))\n",
    "\n",
    "print(\"\\nAfter moving and rescaling:\")\n",
    "verts = mesh.verts_packed()\n",
    "print(\"Center: \", verts.mean(dim=0))\n",
    "print(\"Scale: \", verts.min(), \"to\", verts.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Settings for Mesh Rendering\n",
    "\n",
    "This section provides an overview of the settings required to render a `Meshes` object\n",
    "effectively:\n",
    "\n",
    "-   `RasterizationSettings`: The `faces_per_pixel` parameter determines the maximum\n",
    "    number of faces that can occupy a single pixel in the rasterized image. Increasing\n",
    "    this value allows for more detailed visualization of the meshes.\n",
    "\n",
    "-   `PointLights`: I introduce a point light into the scene at the coordinates\n",
    "    `[0.0, 3.0, 10.0]`. It's important to note that currently, PyTorch3D only supports\n",
    "    lighting configurations for\n",
    "    [mesh rendering](https://github.com/facebookresearch/pytorch3d/issues/1203).\n",
    "\n",
    "-   `BlendParams`: It supports adjustments to the background color.\n",
    "\n",
    "-   `Materials`: Not sure what it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "raster_settings = RasterizationSettings(image_size=image_size, blur_radius=0.0, faces_per_pixel=5)\n",
    "lights = PointLights(location=[[0.0, 3.0, 10.0]], device=device)\n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(0.5, 0.5, 0.5))\n",
    "materials = Materials(device=device, specular_color=[[1.0, 0.0, 0.0]], shininess=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Camera Models\n",
    "\n",
    "I first choose `FoVPerspectiveCameras` to render the scene.\n",
    "\n",
    "Generating _Extrinsic_ Parameters\n",
    "\n",
    "Two primary functions facilitate the generation of _extrinsic_ parameters, essential for\n",
    "transforming points from world coordinates to camera coordinates:\n",
    "\n",
    "-   `look_at_view_transform` function calculates the extrinsic parameters (rotation and\n",
    "    translation) needed for this transformation. It simulates the movement of a camera\n",
    "    within the world frame, ensuring it points towards the world's origin. The function\n",
    "    parameters, distance from the world's origin (`dist`), elevation angle above the XZ\n",
    "    plane (`elev`), and azimuth angle relative to the +Z direction (`azim`), define the\n",
    "    camera's position and orientation in the world frame.\n",
    "\n",
    "-   `look_at_rotation` function, on the other hand, provides a more intuitive approach\n",
    "    to setting the camera's extrinsic parameters. It requires the `camera_position`\n",
    "    vector, indicating the camera's location in world coordinates, and two vectors, at\n",
    "    and up, which represent the position of the object and the world coordinate system's\n",
    "    upward direction, respectively. Given that we have already moved and rescaled the\n",
    "    object to be near the world's origin, using `look_at_rotation` might offer a clearer\n",
    "    method for specifying the camera's position relative to the scene.\n",
    "\n",
    "**Important Note:** There are two things I don't quite understand yet. Fist, I don't\n",
    "understand that why `look_at_rotation` doesn't return translation vector like\n",
    "`look_at_view_transform`. I use the `translation_vector` generated by\n",
    "`look_at_view_transform` to form the extrinsic parameters for `look_at_rotation`.\n",
    "Second, the `translation_vector` returned by `look_at_view_transform` remains the same\n",
    "for various azimuth values (0, 30, and 180 degrees).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuths = [0, 30, 180]\n",
    "cam_pos = [(0, 0, 3), (2.12, 0, 2.12), (1.73, 1.73, 1.73)]\n",
    "view_distance = 3\n",
    "\n",
    "world_to_camera = []\n",
    "\n",
    "for azim in azimuths:\n",
    "    R, translation_vector = look_at_view_transform(\n",
    "        dist=view_distance, elev=0, azim=azim\n",
    "    )\n",
    "    print(\"\\nlook_at_view_transform rotation: \\n\", R)\n",
    "    print(\"look_at_view_transform translation: \", translation_vector)\n",
    "    world_to_camera.append((R, translation_vector))\n",
    "\n",
    "for pos in cam_pos:\n",
    "    p = torch.from_numpy(np.array(pos)).unsqueeze(0).float()\n",
    "    R = look_at_rotation(camera_position=p, at=((0, 0, 0),), up=((0, 1, 0),))\n",
    "    print(\"\\nlook_at_rotation rotation: \\n\", R)\n",
    "    world_to_camera.append((R, translation_vector))\n",
    "\n",
    "cameras = []\n",
    "for r, t in world_to_camera:\n",
    "    fov_persp_cam = FoVPerspectiveCameras(\n",
    "        znear=1.0,\n",
    "        zfar=10,\n",
    "        aspect_ratio=1.0,\n",
    "        fov=45.0,\n",
    "        R=r,\n",
    "        T=t,\n",
    "        degrees=True,\n",
    "        device=device,\n",
    "    )\n",
    "    cameras.append(fov_persp_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Mesh Renderer\n",
    "\n",
    "The `MeshRenderer` can be assembled using the previously defined components. It is\n",
    "capable of batch processing multiple camera models simultaneously; however, this method\n",
    "significantly increases GPU memory consumption. To manage resources efficiently, I\n",
    "recommended to render using a single camera model at a time, proceeding iteratively.\n",
    "\n",
    "Additionally, since `MeshRenderer` inherits from `torch.nn.Module`, rendering a mesh and\n",
    "generating the corresponding image can be accomplished by calling `renderer()`. This\n",
    "function internally invokes the `forward` method to perform the inference operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "\n",
    "for i, azim in enumerate(azimuths):\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(\n",
    "            cameras=cameras[i + 0], raster_settings=raster_settings\n",
    "        ),\n",
    "        shader=SoftPhongShader(\n",
    "            device=device,\n",
    "            cameras=cameras[i + 0],\n",
    "            lights=lights,\n",
    "            blend_params=blend_params,\n",
    "            materials=materials,\n",
    "        ),\n",
    "    )\n",
    "    image = renderer(mesh)\n",
    "    img = image[0, ..., :3].cpu().numpy()\n",
    "    print(f\"Rendered image's shape: {img.shape}.\")\n",
    "    ax[0, i].set_title(f\"look_at_view_transform. ele: 0, azim: {azim}\", fontsize=12)\n",
    "    ax[0, i].imshow(img)\n",
    "    ax[0, i].axis(\"off\")\n",
    "\n",
    "for i, pos in enumerate(cam_pos):\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(\n",
    "            cameras=cameras[i + 3], raster_settings=raster_settings\n",
    "        ),\n",
    "        shader=SoftPhongShader(\n",
    "            device=device,\n",
    "            cameras=cameras[i + 3],\n",
    "            lights=lights,\n",
    "            blend_params=blend_params,\n",
    "            materials=materials,\n",
    "        ),\n",
    "    )\n",
    "    image = renderer(mesh)\n",
    "    img = image[0, ..., :3].cpu().numpy()\n",
    "    print(f\"Rendered image's shape: {img.shape}.\")\n",
    "    ax[1, i].set_title(f\"look_at_rotation. camera in world: {pos}\", fontsize=12)\n",
    "    ax[1, i].imshow(img)\n",
    "    ax[1, i].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "From the rendering outcomes, considering the point light's position at\n",
    "`(0.0, 3.0, 10.0)` in world coordinates, I can deduce the following:\n",
    "\n",
    "-   The top-left image shows the camera is looking at Iron Man's front side. Given that\n",
    "    the point light originates from the +Z direction and the camera also captures this\n",
    "    front view, it suggests Iron Man is oriented towards +Z, with +Z extending outward\n",
    "    from the screen. This orientation implies the camera is facing the -Z direction.\n",
    "\n",
    "-   The top-middle image indicates an azimuth of 30 degrees, which is the angle between\n",
    "    the vector (from the origin to the camera) and the +Z axis.\n",
    "\n",
    "-   The top-right image reveals that the camera is viewing the rear side of Iron Man,\n",
    "    resulting in a darker appearance, because the camera is oriented towards the +Z\n",
    "    direction.\n",
    "\n",
    "-   Now, please reviewing the second-row images, evaluate whether the renderings align\n",
    "    logically with the camera's specified world coordinates, ensuring consistency in the\n",
    "    visual representation based on the camera's positioning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Perspective and Orthographic Projections\n",
    "\n",
    "In orthographic projection, lines that are parallel in the real world remain parallel\n",
    "after the transformation. This type of projection can be described through an affine\n",
    "transformation, which does not account for the perspective distortion seen in\n",
    "perspective projection. This makes orthographic projection particularly useful for\n",
    "technical and engineering drawings where accuracy of parallel lines is crucial.\n",
    "\n",
    "For this comparison, I am using both `FoVPerspectiveCameras` and\n",
    "`FoVOrthographicCameras`, setting the azimuth to 0 and 180 degrees to observe the\n",
    "differences in how these projections render the same scene.\n",
    "\n",
    "**TODO:** Expand the comparison to include PerspectiveCameras, OrthographicCameras, and\n",
    "a fisheye lens model to cover a broader range of camera models and their impact on 3D\n",
    "rendering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_type = [\n",
    "    functools.partial(\n",
    "        FoVPerspectiveCameras,\n",
    "        znear=1.0,\n",
    "        zfar=10,\n",
    "        aspect_ratio=1.0,\n",
    "        fov=60.0,\n",
    "        degrees=True,\n",
    "        device=device,\n",
    "    ),\n",
    "    functools.partial(\n",
    "        FoVOrthographicCameras,\n",
    "        znear=1.0,\n",
    "        zfar=10,\n",
    "        max_y=1.5,\n",
    "        min_y=-1.5,\n",
    "        max_x=1.5,\n",
    "        min_x=-1.5,\n",
    "        device=device,\n",
    "    ),\n",
    "]\n",
    "\n",
    "view_distances = [2, 3]\n",
    "\n",
    "cameras = []\n",
    "world_to_view = []\n",
    "for dist in view_distances:\n",
    "    R, T = look_at_view_transform(dist=dist, elev=0, azim=0)\n",
    "    cameras.append([cam_type(R=R, T=T) for cam_type in camera_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "\n",
    "for i, dist in enumerate(view_distances):\n",
    "    for j, cam_type in enumerate([\"perspective\", \"orthographic\"]):\n",
    "        camera = cameras[i][j]\n",
    "        renderer = MeshRenderer(\n",
    "            rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings),\n",
    "            shader=SoftPhongShader(\n",
    "                device=device,\n",
    "                cameras=camera,\n",
    "                lights=lights,\n",
    "                blend_params=blend_params,\n",
    "                materials=materials,\n",
    "            ),\n",
    "        )\n",
    "        image = renderer(mesh)\n",
    "        img = image[0, ..., :3].cpu().numpy()\n",
    "        print(f\"Rendered image's shape: {img.shape}.\")\n",
    "        ax[i, j].set_title(f\"{cam_type} projection. View distance: {dist}\", fontsize=12)\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "In perspective projection, the distance between the object and the observer is crucial;\n",
    "objects appear larger as the viewing distance decreases. This effect is due to the\n",
    "perspective projection mimicking the way the human eye perceives depth and distance,\n",
    "causing objects closer to the viewer to appear larger than those further away.\n",
    "\n",
    "Conversely, in orthographic projection, the size of the object remains consistent\n",
    "regardless of the viewing distance. This projection does not account for depth in the\n",
    "same way, leading to a uniform representation of the object's size. Additionally, you\n",
    "can see that objects positioned behind Iron Man are more obscured in perspective\n",
    "projection than in orthographic projection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering a Rotating Iron Man\n",
    "\n",
    "Now, I will render a rotating Iron Man model under consistent lighting conditions. To\n",
    "achieve a brightly lit model throughout the rotation, it's essential to adjust the\n",
    "position of the light source to match the camera's movement. The concept here is to\n",
    "maintain the illumination on the Iron Man model as if the light source is always facing\n",
    "him, even as the camera angle changes.\n",
    "\n",
    "Initially, when the azimuth is set to 0, the camera is oriented towards the -Z\n",
    "direction. Correspondingly, we'll position the light source in the -Z direction as well,\n",
    "at `[0.0, 3.0, -10.0]`. As Iron Man rotates, or as the camera's viewpoint changes, we'll\n",
    "apply a rotation matrix `R` to the light source. This matrix will mirror the rotation\n",
    "applied to Iron Man or the camera's movement around him, ensuring that the model remains\n",
    "well-lit from the camera's perspective at all times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 120\n",
    "light_init_pos = [0, 3, -10]\n",
    "images = []\n",
    "\n",
    "for azim in torch.linspace(0, 360, batch_size):\n",
    "    R, T = look_at_view_transform(dist=2.7, elev=0, azim=azim)\n",
    "    light_pos_rotate = np.dot(R, light_init_pos)\n",
    "    light = PointLights(device=device, location=light_pos_rotate.tolist())\n",
    "    cameras = FoVPerspectiveCameras(\n",
    "        znear=1.0,\n",
    "        zfar=3.0,\n",
    "        aspect_ratio=1.0,\n",
    "        fov=45.0,\n",
    "        degrees=True,\n",
    "        R=R,\n",
    "        T=T,\n",
    "        device=device,\n",
    "    )\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
    "        shader=SoftPhongShader(\n",
    "            device=device, cameras=cameras, lights=light, blend_params=blend_params\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    img = renderer(mesh, materials=materials)[0, ..., :3].cpu().numpy()\n",
    "    img *= 255\n",
    "    images.append(img.astype(np.uint8))\n",
    "\n",
    "imageio.mimsave(OUTPUT_DIR / \"ironman_mesh.gif\", images, fps=12)\n",
    "\n",
    "# NOTE: The following warning shows:\n",
    "# Bin size was too small in the coarse rasterization phase. This caused an overflow,\n",
    "# meaning output may be incomplete. To solve, try increasing max_faces_per_bin /\n",
    "# max_points_per_bin, decreasing bin_size, or setting bin_size to 0 to use the naive\n",
    "# rasterization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Clouds Representation\n",
    "\n",
    "Point clouds are a fundamental representation in computer vision and 3D modeling,\n",
    "capturing the essence of a physical space or object through a collection of points in a\n",
    "three-dimensional coordinate system. Each point in a point cloud is represented by a\n",
    "3-vector $(x, y, z)$, indicating its position in space:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\in \\mathbb{R}^3: \\quad \\mathbf{v} = \\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Beyond mere coordinates, point clouds often encompass additional attributes that enrich\n",
    "the data. Commonly, these include RGB color information and normals, which represent the\n",
    "orientation of each point. Normals are crucial for understanding the surface geometry\n",
    "and are represented as another 3-vector in $\\mathbb{R}^3$. The inclusion of RGB values\n",
    "adds visual detail to the point cloud, allowing for a more accurate and vivid\n",
    "reconstruction of the original scene or object:\n",
    "\n",
    "$$\n",
    "\\text{RGB color: } \\mathbf{c} = \\begin{pmatrix}\n",
    "R \\\\\n",
    "G \\\\\n",
    "B\n",
    "\\end{pmatrix}, \\quad n = \\begin{pmatrix}\n",
    "n_x \\\\\n",
    "n_y \\\\\n",
    "n_z\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Point clouds are inherently unordered, meaning there is no intrinsic sequence to the\n",
    "points. This characteristic poses unique challenges and considerations for processing\n",
    "and analyzing point cloud data.\n",
    "\n",
    "In this section, I render a point cloud of Moszna Castle in Poland. An interesting\n",
    "aspect of this dataset is that the RGB color is encoded as `np.uint16`, which needs to\n",
    "be converted to floating-point scale between 0 and 1. This conversion is done by\n",
    "dividing the RGB values by $2^{16} - 1$:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{Normalized RGB: } \\mathbf{c}_{\\text{norm}} = \\frac{\\mathbf{c}}{2^{16} - 1}\n",
    "$$\n",
    "\n",
    "Additionally, similar to mesh processing, we manipulate the point cloud to center it\n",
    "near the origin (0,0,0) and rescale its position. This normalization step facilitates\n",
    "easier manipulation, visualization, and further processing of the point cloud data,\n",
    "ensuring consistency in scale and orientation across different datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_file = DATA_DIR / \"Palac_Moszna/Palac_Moszna.laz\"\n",
    "\n",
    "# Open the LAS/LAZ file\n",
    "las = laspy.read(pc_file)\n",
    "print(f\"Read in *.las/*laz file as {type(las)}\")\n",
    "print(f\"The shape of xyz coordinates of point cloud is {las.xyz.shape}.\")\n",
    "print(f\"The range of R channel ({las.red.dtype}): {las.red.min()} to {las.red.max()}\")\n",
    "print(\n",
    "    f\"The range of G channel ({las.green.dtype}): {las.green.min()} to {las.green.max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"The range of B channel ({las.blue.dtype}): {las.blue.min()} to {las.blue.max()}\"\n",
    ")\n",
    "assert (\n",
    "    las.xyz.shape[0] == las.red.shape[0]\n",
    "    and las.xyz.shape[0] == las.green.shape[0]\n",
    "    and las.xyz.shape[0] == las.blue.shape[0]\n",
    ")\n",
    "\n",
    "# Read xyz as tensor\n",
    "vertices = torch.Tensor(las.xyz).to(device)\n",
    "\n",
    "print(\"\\nBefore moving and rescaling points:\")\n",
    "center = vertices.mean(dim=0)\n",
    "scale = max((vertices - center).abs().max(0)[0])\n",
    "print(\"Center: \", center)\n",
    "print(\"Scale: \", vertices.min(), \"to\", vertices.max())\n",
    "\n",
    "vertices -= center\n",
    "vertices /= float(scale)\n",
    "print(\"\\nAfter moving and rescaling points:\")\n",
    "print(\"Center: \", vertices.mean(dim=0))\n",
    "print(\"Scale: \", vertices.min(), \"to\", vertices.max())\n",
    "\n",
    "\n",
    "# Scale RGB to [0, 1] and read RGB as tensor\n",
    "print(\"\\n Scaling RGB:\")\n",
    "rgb = np.stack([las.red, las.green, las.blue], axis=1) / (2**16 - 1)\n",
    "rgb = torch.Tensor(rgb).to(device)\n",
    "print(f\"The range of RGB is from {rgb.min()} to {rgb.max()}.\")\n",
    "\n",
    "# Create pointcloud object\n",
    "pointcloud = Pointclouds(points=[vertices], features=[rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Rendering Settings\n",
    "\n",
    "When configuring the rendering settings for a point cloud, several key parameters play a\n",
    "crucial role in determining the visual quality and clarity of the rendered image.\n",
    "\n",
    "-   `image_size`: The dimensions of the output image are set to 512x512 pixels.\n",
    "\n",
    "-   `radius`: The radius parameter, expressed in Normalized Device Coordinates (NDC),\n",
    "    defines the size of each point (represented as a disk in the rendering process) to\n",
    "    be rasterized. The choice of radius affects the visual density and overlap of points\n",
    "    in the rendered image. A larger radius results in more substantial point overlaps,\n",
    "    contributing to a smoother appearance but potentially obscuring finer details.\n",
    "\n",
    "-   `points_per_pixel`: This parameter specifies the maximum number of points considered\n",
    "    for each pixel in the rendered image. Increasing `points_per_pixel` enhances the\n",
    "    rendering's smoothness and reduces noise by allowing more points to contribute to\n",
    "    the color and opacity of each pixel. However, higher values also increase the\n",
    "    computational load and memory usage during rendering.\n",
    "\n",
    "-   `background_color`: The background color is set to gray `[0.5, 0.5, 0.5]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "raster_settings = PointsRasterizationSettings(image_size=image_size, radius=0.005, points_per_pixel=150)\n",
    "alpha_comp = AlphaCompositor(background_color=(0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First View of the Point Cloud\n",
    "\n",
    "For the initial visualization of the point cloud, I employ `FoVOrthographicCameras` with\n",
    "an azimuth set to 0 degrees and different elevation values: 0, -60, -89, and -90\n",
    "degrees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_view = 3\n",
    "elevation = [0, -60, -89, -90] # Using -90 will flip the result.\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(elevation), figsize=(20, 5))\n",
    "\n",
    "for i, elev in enumerate(elevation):\n",
    "    R, T = look_at_view_transform(dist=distance_view, elev=elev, azim=0)\n",
    "    print(f\"\\nElevation: {elev} degrees.\")\n",
    "    print(\"R: \", R)\n",
    "    print(\"T: \", T)\n",
    "    cameras = FoVOrthographicCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        min_x=-1,\n",
    "        max_x=1,\n",
    "        min_y=-1,\n",
    "        max_y=1,\n",
    "        znear=0.01,\n",
    "        zfar=10,\n",
    "        device=device,\n",
    "    )\n",
    "    rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "    renderer = PointsRenderer(rasterizer=rasterizer, compositor=alpha_comp)\n",
    "\n",
    "    images = renderer(pointcloud)\n",
    "    img = images[0, ..., :3].cpu().numpy()\n",
    "    print(f\"The image's shape: {img.shape}, type: {img.dtype}, min: {img.min()}, max:{img.max()}\")\n",
    "    ax[i].set_title(f\"Elevation: {elev} deg\", fontsize=12)\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "The leftmost image, where the camera points towards the -Z direction, offers a top-down\n",
    "perspective of the castle. This setup indicates that the vector from the castle's base\n",
    "to its apex (the highest point) is aligned with the +Z direction. Viewing the castle\n",
    "from this angle mirrors the traditional topographic and aerial views, providing a\n",
    "comprehensive overlook from the zenith.\n",
    "\n",
    "The transition in the camera's elevation from -89 to -90 degrees results in an\n",
    "**upside-down** rendering, which I am not sure why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_z(theta):  # Rotation around the z-axis\n",
    "    theta = np.deg2rad(theta)\n",
    "    return np.array(\n",
    "        [\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def rotation_y(theta):  # Rotation around the y-axis\n",
    "    theta = np.deg2rad(theta)\n",
    "    return np.array(\n",
    "        [\n",
    "            [np.cos(theta), 0, np.sin(theta)],\n",
    "            [0, 1, 0],\n",
    "            [-np.sin(theta), 0, np.cos(theta)],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def rotation_x(theta):  # Rotation around the x-axis\n",
    "    theta = np.deg2rad(theta)\n",
    "    return np.array(\n",
    "        [\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(theta), -np.sin(theta)],\n",
    "            [0, np.sin(theta), np.cos(theta)],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing World-to-Camera Extrinsic Parameters Manually\n",
    "\n",
    "To achieve a more comprehensive view of the scene that departs from the top-down\n",
    "perspective, constructing extrinsic parameters manually becomes necessary. The process\n",
    "involves rotating the camera around the x-axis and setting a specific translation\n",
    "vector.\n",
    "\n",
    "Step 1: Rotating the Camera Around the X-Axis\n",
    "\n",
    "Rotating the camera around the x-axis alters the elevation angle, offering a different\n",
    "perspective of the scene. The rotation matrix for an angle $\\theta$ around the x-axis\n",
    "can be represented as:\n",
    "\n",
    "$$\n",
    "R_x(\\theta) = \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "0 & \\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Step 2: Defining the Translation Vector\n",
    "\n",
    "After rotating the camera, setting the translation vector to `(0, 0, 3)` repositions the\n",
    "camera's perspective in relation to the scene. This translation means that a point\n",
    "located at `(0, 0, 0)` in world coordinates will be perceived as `(0, 0, 3)` in camera\n",
    "coordinates. Essentially, this moves the camera's viewpoint 3 units away from the origin\n",
    "along the z-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the angles\n",
    "theta_z_degrees = [0, 90]\n",
    "theta_x_degrees = [0, 90, 120]\n",
    "\n",
    "T = torch.zeros(3).unsqueeze(dim=0)  # 1D to 2D\n",
    "T[0, 2] = 3\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(theta_z_degrees), ncols=len(theta_x_degrees), figsize=(20, 13))\n",
    "\n",
    "for i, theta_z in enumerate(theta_z_degrees):\n",
    "    for j, theta_x in enumerate(theta_x_degrees):\n",
    "        rotation = np.dot(rotation_z(theta_z), rotation_x(theta_x))\n",
    "        R = torch.tensor(rotation).to(device=device).unsqueeze(dim=0)\n",
    "        cameras = FoVOrthographicCameras(\n",
    "            R=R,\n",
    "            T=T,\n",
    "            min_x=-0.8,\n",
    "            max_x=0.8,\n",
    "            min_y=-0.8,\n",
    "            max_y=0.8,\n",
    "            znear=0.01,\n",
    "            zfar=10,\n",
    "            device=device,\n",
    "        )\n",
    "        rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "        renderer = PointsRenderer(rasterizer=rasterizer, compositor=alpha_comp)\n",
    "\n",
    "        images = renderer(pointcloud)\n",
    "        img = images[0, ..., :3].cpu().numpy()\n",
    "        print(f\"The image's shape: {img.shape}, type: {img.dtype}, min: {img.min()}, max:{img.max()}\")\n",
    "        ax[i, j].set_title(f\"X-axis rotation: {theta_x}, Z-axis rotation: {theta_z}\", fontsize=12)\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "The top-left image may seem like we are viewing the castle from above, but in reality,\n",
    "the perspective is from below, directed towards the +Z direction. This understanding\n",
    "aligns with the earlier explanation that the castle's apex points towards the +Z\n",
    "direction.\n",
    "\n",
    "Consequently, to observe the castle's surface directly, the camera must rotate beyond\n",
    "the 90 degrees around x-axis. However, determining whether this rotation should be 90\n",
    "degrees or -90 degrees is difficult, especially since the point cloud itself remains\n",
    "static, and it is the camera that moves.\n",
    "\n",
    "Now, please inspect every image to see if the rendering corresponds accurately with the\n",
    "specified settings of $\\theta_x$ and $\\theta_z$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size - this is the number of different viewpoints from which we want to render the mesh.\n",
    "batch_size = 120\n",
    "angles = torch.linspace(0, 360, batch_size)\n",
    "\n",
    "theta_x = 120\n",
    "\n",
    "images = []\n",
    "for ang in angles:\n",
    "    rotation = np.dot(rotation_z(ang), rotation_x(theta_x))\n",
    "    R = torch.tensor(rotation).to(device=device).unsqueeze(dim=0)\n",
    "\n",
    "    camera = FoVOrthographicCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        min_x=-0.8,\n",
    "        max_x=0.8,\n",
    "        min_y=-0.8,\n",
    "        max_y=0.8,\n",
    "        znear=0.01,\n",
    "        zfar=10,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    rasterizer = PointsRasterizer(cameras=camera, raster_settings=raster_settings)\n",
    "    renderer = PointsRenderer(rasterizer=rasterizer, compositor=alpha_comp)\n",
    "\n",
    "    img = renderer(pointcloud).cpu().numpy()[0, ..., :3]\n",
    "    img *= 255\n",
    "    images.append(img.astype(np.uint8))\n",
    "\n",
    "imageio.mimsave(OUTPUT_DIR / \"Palac_Moszna_pointcloud.gif\", images, fps=12)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxels Representation\n",
    "\n",
    "Voxels, short for volumetric pixels, represent a value on a regular grid in\n",
    "three-dimensional space. Unlike pixels, which cover 2D surface areas, voxels extend into\n",
    "3D, offering depth. Each voxel can be in one of two states: 0 or 1, indicating whether\n",
    "it occupies space or not. This binary voxel grid approach simplifies modeling the\n",
    "presence or absence of material in 3D environments.\n",
    "\n",
    "In addition to their binary state, voxels can hold more complex data such as density and\n",
    "color, enhancing the detail and realism of 3D models. Both density and color values are\n",
    "normalized between `[0, 1]`, where 0 represents the absence of material or color, and 1\n",
    "signifies the maximum density or full intensity of color.\n",
    "\n",
    "In PyTorch3D, the `Volumes` class facilitates the creation of a batch of volume objects,\n",
    "utilizing `densities` and `features` as its primary parameters. The `densities`\n",
    "parameter is designed to encapsulate different measures of volume cell \"density,\" which\n",
    "can include concepts like opacity or signed/unsigned distances from the nearest surface.\n",
    "In the context of our cow voxels, densities are used to indicate these various opacity\n",
    "measures. The features parameter, on the other hand, typically carries additional\n",
    "attributes such as colors.\n",
    "\n",
    "Note: I've previously executed the\n",
    "[`fit_textured_volume.ipynb` notebook](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/fit_textured_volume.ipynb)\n",
    "to create voxel data for a cow model, which is now stored in\n",
    "\"cow_mesh/cow_voxel_grid.npz\". This tutorial will concentrate on the rendering of\n",
    "voxels, bypassing the model fitting process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_file = DATA_DIR / \"cow_mesh/cow_voxel_grid.npz\"\n",
    "\n",
    "# Load point cloud\n",
    "# the density and color has been applied to sigmoid function.\n",
    "voxels = np.load(voxel_file)\n",
    "densities = voxels[\"densities\"]\n",
    "colors = voxels[\"colors\"]\n",
    "voxelsize = voxels[\"voxelsize\"]  # a scalar array can be converted to a scalar value\n",
    "render_size = voxels[\"render_size\"].item()\n",
    "volume_extent_world = voxels[\"volume_extent_world\"].item()\n",
    "\n",
    "densities = torch.Tensor(densities).to(device)\n",
    "colors = torch.Tensor(colors).to(device)\n",
    "voxelsize = torch.Tensor(voxelsize)\n",
    "\n",
    "print(densities.shape, densities.min(), densities.max())\n",
    "print(colors.shape, colors.min(), colors.max())\n",
    "print(\"voxelsize: \", voxelsize)\n",
    "print(\"volume_extent_world: \", volume_extent_world)\n",
    "\n",
    "\n",
    "image_size = 512\n",
    "raysampler = NDCMultinomialRaysampler(\n",
    "    image_width=image_size,\n",
    "    image_height=image_size,\n",
    "    n_pts_per_ray=350,  # maximum value if bigger, memory error\n",
    "    # n_rays_per_image=\n",
    "    # n_rays_total=\n",
    "    min_depth=0.01,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "renderer = VolumeRenderer(\n",
    "    raysampler=raysampler,\n",
    "    raymarcher=EmissionAbsorptionRaymarcher(),\n",
    ")\n",
    "R, T = look_at_view_transform(dist=2.7, elev=0, azim=180)\n",
    "camera = FoVPerspectiveCameras(\n",
    "    R=R,\n",
    "    T=T,\n",
    "    znear=0.01,\n",
    "    zfar=100,\n",
    "    aspect_ratio=1,\n",
    "    fov=50,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n",
    "for i, density_factor in enumerate([0.1, 1]):\n",
    "    volumes = Volumes(\n",
    "        densities=[density_factor * densities],\n",
    "        features=[colors],\n",
    "        voxel_size=voxelsize,\n",
    "    )\n",
    "    rendered_images, rendered_silhouettes = renderer(cameras=camera, volumes=volumes)\n",
    "    img = rendered_images[0, ..., :3].cpu().numpy()\n",
    "    ax[i].set_title(f\"density_factor: {density_factor}\", fontsize=12)\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Volume Rendering Settings\n",
    "\n",
    "Parameters of NDCMultinomialRaysampler:\n",
    "\n",
    "-   `image_size`: Specifies the dimensions of the output image (width and height) in\n",
    "    pixels. This determines the resolution of the ray grid.\n",
    "-   `n_pts_per_ray`: The number of points to sample along each ray. More points allow\n",
    "    for a more detailed rendering of the volume but increase computation time.\n",
    "-   `min_depth`: The minimum depth (in NDC) from which to start sampling along each ray.\n",
    "    This parameter helps in clipping the volume space to start rendering from a specific\n",
    "    depth.\n",
    "-   `max_depth`: The maximum depth (in NDC) up to which to sample along each ray,\n",
    "    allowing the clipping of the volume space at the far end.\n",
    "\n",
    "NOTE: At the moment, the `VolumeRenderer`\n",
    "[does not support direct lighting](https://github.com/facebookresearch/pytorch3d/issues/1203).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "raysampler = NDCMultinomialRaysampler(\n",
    "    image_width=image_size,\n",
    "    image_height=image_size,\n",
    "    n_pts_per_ray=400,  # maximum value if bigger, memory error\n",
    "    min_depth=0.01,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "renderer = VolumeRenderer(\n",
    "    raysampler=raysampler,\n",
    "    raymarcher=raymarcher,\n",
    ")\n",
    "\n",
    "elevations = [-30, 0, 30]\n",
    "view_distances = [2, 2.7, 3.5]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=len(elevations), ncols=len(view_distances), figsize=(15, 15)\n",
    ")\n",
    "\n",
    "for i, dist in enumerate(view_distances):\n",
    "    for j, elev in enumerate(elevations):\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elev, azim=180)\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R,\n",
    "            T=T,\n",
    "            znear=0.01,\n",
    "            zfar=100,\n",
    "            aspect_ratio=1,\n",
    "            fov=50,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        rendered_images, rendered_silhouettes = renderer(\n",
    "            cameras=camera, volumes=volumes\n",
    "        )\n",
    "\n",
    "        img = rendered_images[0, ..., :3].cpu().numpy()\n",
    "        ax[i, j].set_title(f\"view_distances: {dist}, Elevations: {elev}\", fontsize=12)\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "In the last row of images, we can observe that voxels located at the rear of the\n",
    "observed volume are not rendered or are \"cut off\" from the viewer's perspective.\n",
    "\n",
    "The `max_depth` parameter in volume rendering settings defines the maximum distance from\n",
    "the viewpoint (or camera) at which rays will sample the volume. Essentially, it sets a\n",
    "boundary in the depth of the scene beyond which no sampling occurs. When the `max_depth`\n",
    "is too small relative to the observer's viewing distance, it restricts the depth range\n",
    "that the rays can explore. As a result, parts of the volume that lie beyond this\n",
    "`max_depth` threshold are not considered in the final rendered image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raysampler = NDCMultinomialRaysampler(\n",
    "    image_width=image_size,\n",
    "    image_height=image_size,\n",
    "    n_pts_per_ray=350,  # maximum value if bigger, memory error\n",
    "    min_depth=0.01,\n",
    "    max_depth=5,\n",
    ")\n",
    "renderer = VolumeRenderer(\n",
    "    raysampler=raysampler,\n",
    "    raymarcher=EmissionAbsorptionRaymarcher(),\n",
    ")\n",
    "\n",
    "batch_size = 120\n",
    "azimuths = torch.linspace(-180, 180, batch_size)\n",
    "\n",
    "images = []\n",
    "for azim in azimuths:\n",
    "    R, T = look_at_view_transform(dist=3, elev=0, azim=azim)\n",
    "    camera = FoVPerspectiveCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        znear=-10,\n",
    "        zfar=100,\n",
    "        aspect_ratio=1,\n",
    "        fov=50,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    rendered_images, rendered_silhouettes = renderer(cameras=camera, volumes=volumes)\n",
    "    img = rendered_images[0, ..., :3].cpu().numpy()\n",
    "    img *= 255\n",
    "    images.append(img.astype(np.uint8))\n",
    "\n",
    "imageio.mimsave(OUTPUT_DIR / \"cow_voxel.gif\", images, fps=12)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_deep_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
